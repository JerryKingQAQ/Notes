# CNN-LSTM模型

## 基于CNN的人脸微表情识别模型

基于 CNN 的微表情识别包括图向量输入层、卷积层、池化层、融合层和输出层.

![](C:\Users\Jerry\Desktop\截图\人工智能\微信截图_20220326215712.png)

### 输入层

在微表情识别中，在输入层一般使用其中均值化操作进行图像预处理，即把输入数据各个维度都中心化到0，所有样本求和求平均，然后用所有的样本减去这个均值样本。为了统一输入数据，首先检测人脸区域，然后裁剪并调整检测到的人脸大小9６×９６像素，调整后的概率密度函数计算式为

![](C:\Users\Jerry\Desktop\截图\人工智能\微信截图_20220326224324.png)

其中，Ｇｋ 为绘图图像，Ｎｋ 为Ｇｋ 出现的次数，Ｎ 为图像中像素的总个数。

### 卷积层

卷积层的功能是对输入数据进行特征提取，其内部包含多个卷积核，组成卷积核的每个元素都对应一个权重系数和一个偏差量。卷积核在工作时，会有规律地扫过输入特征，在感受野内对输入特征做矩阵元素乘法求和并叠加偏差量：si = f (ω ∗ ai:i+h−1 + b) ,

其中，ω∈Rh×d 为卷积核大小，d 为图像向量维度，a 为卷积窗口，b 为偏置项，f 为激活函数. 经过卷积操作，每个输入文本得到一个大小为 m−h+1 的特征矩阵 S.

### 池化层

池化层对于输入的 S 采用 MaxPooling 进行压缩，保留特征矩阵 S 的主特征，同时减少参数和计算量.

### 全连接层

全连接层位于卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去空间拓扑结构，被展开为向量并通过激励函数。全连接层的作用是对提取的特征进行非线性组合以得到输出。

### 输出层

对于微表情识别，输出层使用归一化指数函数（softmax function）输出分类标签，最后得出计算结果。

## 基于LSTM的人脸微表情识别模型

传统的循环神经网络（RNN）在输入数据过多时，因为非线性变换，在尾部的梯度进行反向传播时不能将梯度传给句子的起始位置，经过训练会出现一个不可忽视的问题:梯度弥散，梯度弥散是一个需要解决的问题。针对这个问题人们提出了一个基于循环神经网络的新的网络模型———LSTM，LSTM可以理解为升级版的循环神经网络。

LSTM 可以很好地解决梯度消失和梯度爆炸的问题，LSTM 元胞中有输入门、输出门和遗忘门。输入门决定输入哪些信息，遗忘门决定保留哪些信息，输出门决定输出哪些信息。

![image-20220326224854580](C:\Users\Jerry\AppData\Roaming\Typora\typora-user-images\image-20220326224854580.png)

下面就几个公式介绍一下LSTM具体的工作流程:

 

第一步:新输入xt前状态ht－1决定C哪些信息可以舍弃ft与Ct－1运算，对部分信息进行去除:

ft=δ(Wf·［ht－1，xt］)+bf      (1)

第二步:新输入xt前状态ht－1告诉C哪些信息想要保存it，新信息添加时的系数(对比ft)C～t单独新数据形式的控制参数，用于对Ct进行更新:

it=δ(Wi·［ht－1，xt］+bi)     (2)

C～t=tanh(Wc·［ht－1，xt］+bc)     (3)

第三步:根据旧的控制参数Ct－1，新生成的更新控制参数C～t，组合生成该时刻最终控制参数:

 Ct=ft×Ct－1+it×C～t      （4）

(4)第四步:根据控制参数Ct产生此刻的新的LSTM输出:

Ot=δ(WO［ht－1，xt］+bO)     (5)

ht=Ot×tanh(Ct)          (6)
